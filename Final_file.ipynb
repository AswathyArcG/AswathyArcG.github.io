{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gczeIWL7Yqml",
    "outputId": "353b3804-fb2b-4ead-d190-69cc7ef11ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\aswat\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5eLSkVqlhh6",
    "outputId": "9cf46693-5e60-425d-8693-22a5df24fea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: presidio-analyzer in c:\\users\\aswat\\anaconda3\\lib\\site-packages (2.2.353)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.4.4 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from presidio-analyzer) (3.7.4)\n",
      "Requirement already satisfied: regex in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from presidio-analyzer) (2022.7.9)\n",
      "Requirement already satisfied: tldextract in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from presidio-analyzer) (3.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from presidio-analyzer) (6.0)\n",
      "Requirement already satisfied: phonenumbers<9.0.0,>=8.12 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from presidio-analyzer) (8.13.32)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.23.5)\n",
      "Requirement already satisfied: idna in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tldextract->presidio-analyzer) (3.4)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tldextract->presidio-analyzer) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tldextract->presidio-analyzer) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->presidio-analyzer) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests-file>=1.4->tldextract->presidio-analyzer) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.4.4->presidio-analyzer) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install presidio-analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XWdmM-gGmHV-",
    "outputId": "42e0e840-89df-4a99-a3d7-8d78fc7beff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\aswat\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: boto3>=1.20.27 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.34.64)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.3.4)\n",
      "Requirement already satisfied: conllu>=4.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (4.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.2.14)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (6.2.0)\n",
      "Requirement already satisfied: gdown>=4.4.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (5.1.0)\n",
      "Requirement already satisfied: gensim>=4.2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (4.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.15.1)\n",
      "Requirement already satisfied: janome>=0.4.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.5.0)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: lxml>=4.8.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (3.7.2)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (10.2.0)\n",
      "Requirement already satisfied: mpld3>=0.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.5.10)\n",
      "Requirement already satisfied: pptree>=3.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: pytorch-revgrad>=0.2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.2.2)\n",
      "Requirement already satisfied: segtok>=1.5.11 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (4.65.0)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (4.32.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (1.26.16)\n",
      "Requirement already satisfied: wikipedia-api>=0.5.7 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (0.6.0)\n",
      "Requirement already satisfied: semver<4.0.0,>=3.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from flair) (3.0.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.64 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (1.34.64)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (0.1.99)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (3.9.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (2.0.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (2023.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (23.1)\n",
      "Requirement already satisfied: six in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from mpld3>=0.3->flair) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (2.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from tqdm>=4.63.0->flair) (0.4.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.3.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (4.25.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2.0.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (0.3.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.28.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2023.3)\n",
      "Requirement already satisfied: simpful in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\aswat\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9FxNt5pZY0e2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'conlleval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconlleval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'conlleval'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import logging\n",
    "from typing import Optional, List, Tuple, Set\n",
    "from presidio_analyzer import (\n",
    "    RecognizerResult,\n",
    "    EntityRecognizer,\n",
    "    AnalysisExplanation,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2ro_nntY-FC"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg0WkejPZBn8"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeMvk_zKZFXy"
   },
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weGmhigxZMaT"
   },
   "outputs": [],
   "source": [
    "conll_data = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEvvIFAgdcAF"
   },
   "outputs": [],
   "source": [
    "def dataset_to_dataframe(dataset):\n",
    "    data_dict = {key: dataset[key] for key in dataset.features}\n",
    "    return pd.DataFrame(data_dict)\n",
    "\n",
    "# Combine all splits (train, validation, test) into a single DataFrame\n",
    "conll_df = pd.concat([dataset_to_dataframe(conll_data[split]) for split in conll_data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "UejgBp-Ng_l_",
    "outputId": "98b45e90-3b08-4857-f7eb-42e9a319eb29"
   },
   "outputs": [],
   "source": [
    "csv_file_path = \"conll_data.csv\"\n",
    "conll_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Download the CSV file to local machine\n",
    "\n",
    "files.download(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRyX6MExgwi7",
    "outputId": "78702706-75b5-4d4e-9cf7-08f21bb99dcb"
   },
   "outputs": [],
   "source": [
    "print(conll_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAiHg17QhO-2",
    "outputId": "065d13c3-c8ea-40f3-f84a-2fc4f2332ff2"
   },
   "outputs": [],
   "source": [
    "print(conll_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LwwJ8zbhVlk",
    "outputId": "c32dde53-bf78-4f94-aa32-1cdef388099e"
   },
   "outputs": [],
   "source": [
    "print(conll_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njbG34F3hl5D",
    "outputId": "81cb8929-f9a0-4a07-d1f6-f306d2ffc7c0"
   },
   "outputs": [],
   "source": [
    "print(conll_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98pX56RShpgR",
    "outputId": "18ef0b75-727b-4f2b-8ae9-6b4415e8e17a"
   },
   "outputs": [],
   "source": [
    "label_counts = conll_df['ner_tags'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "Yd71HpRQhuoZ",
    "outputId": "066c4b14-3edf-4139-e665-cdbf95dac172"
   },
   "outputs": [],
   "source": [
    "top_10_labels = label_counts.head(10)\n",
    "\n",
    "# Plot the distribution of the top 10 NER tags\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10_labels.plot(kind='bar')\n",
    "plt.title('Top 10 Most Common NER Tags')\n",
    "plt.xlabel('NER Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQgmkV1fZRhI"
   },
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            if len(tokens) > 0:\n",
    "                f.write(\n",
    "                    str(len(tokens))\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(tokens)\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(map(str, ner_tags))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdufhIrEZRs2",
    "outputId": "09e10fc1-6fdf-4281-ac81-973d32dad3a5"
   },
   "outputs": [],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7T9RCZ3ZSKB",
    "outputId": "c2dae2fc-b812-4d64-b3eb-23e2d38710c3"
   },
   "outputs": [],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdcDo5IJZfjl"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fXqLG3FZfmx",
    "outputId": "42354174-a397-4b9e-eda0-4b1d5ed62665"
   },
   "outputs": [],
   "source": [
    "print(list(train_data.take(1).as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtt-G6ezZto5"
   },
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "\n",
    "# We use `padded_batch` here because each record in the dataset has a\n",
    "# different length.\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqCmpwqgZtrs"
   },
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction= 'none'\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQDGyN4gZtuC",
    "outputId": "5b743bb3-2112-47b2-e4f7-0db45991f93d"
   },
   "outputs": [],
   "source": [
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)\n",
    "\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"eu rejects german call to boycott british lamb\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPPszQFIcEKi",
    "outputId": "22d8a103-b1d1-402b-b401-f5662fdaca00"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x, verbose=0)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)\n",
    "\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BX6jui33cEiJ",
    "outputId": "91207f20-c00e-46ab-ae91-9bc1dfc8d804"
   },
   "outputs": [],
   "source": [
    "def test_model_with_input(ner_model, mapping):\n",
    "    # Get input sentence from user\n",
    "    input_sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "    # Tokenize and convert input sentence to IDs\n",
    "    sample_input = tokenize_and_convert_to_ids(input_sentence)\n",
    "    sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "\n",
    "    # Predict tags using the trained model\n",
    "    output = ner_model.predict(sample_input)\n",
    "    predictions = np.argmax(output, axis=-1)[0]\n",
    "    predicted_tags = [mapping[i] for i in predictions]\n",
    "\n",
    "    # Print the predicted tags for each token in the input sentence\n",
    "    print(\"Input sentence:\", input_sentence)\n",
    "    print(\"Predicted tags:\", predicted_tags)\n",
    "\n",
    "# Test the model with user input\n",
    "test_model_with_input(ner_model, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWwGi143lCVF"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"presidio-analyzer\")\n",
    "\n",
    "\n",
    "class FlairRecognizer(EntityRecognizer):\n",
    "    \"\"\"\n",
    "    Wrapper for a flair model, if needed to be used within Presidio Analyzer.\n",
    "    :example:\n",
    "    >from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "    >flair_recognizer = FlairRecognizer()\n",
    "    >registry = RecognizerRegistry()\n",
    "    >registry.add_recognizer(flair_recognizer)\n",
    "    >analyzer = AnalyzerEngine(registry=registry)\n",
    "    >results = analyzer.analyze(\n",
    "    >    \"My name is Christopher and I live in Irbid.\",\n",
    "    >    language=\"en\",\n",
    "    >    return_decision_process=True,\n",
    "    >)\n",
    "    >for result in results:\n",
    "    >    print(result)\n",
    "    >    print(result.analysis_explanation)\n",
    "    \"\"\"\n",
    "\n",
    "    ENTITIES = [\n",
    "        \"LOCATION\",\n",
    "        \"PERSON\",\n",
    "        \"ORGANIZATION\",\n",
    "        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n",
    "    ]\n",
    "\n",
    "    DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"\n",
    "\n",
    "    CHECK_LABEL_GROUPS = [\n",
    "        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n",
    "        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n",
    "        ({\"ORGANIZATION\"}, {\"ORG\"}),\n",
    "        # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII\n",
    "    ]\n",
    "\n",
    "    MODEL_LANGUAGES = {\"en\": \"flair/ner-english-large\"}\n",
    "\n",
    "    PRESIDIO_EQUIVALENCES = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"LOCATION\",\n",
    "        \"ORG\": \"ORGANIZATION\",\n",
    "        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        supported_language: str = \"en\",\n",
    "        supported_entities: Optional[List[str]] = None,\n",
    "        check_label_groups: Optional[Tuple[Set, Set]] = None,\n",
    "        model: SequenceTagger = None,\n",
    "        model_path: Optional[str] = None,\n",
    "    ):\n",
    "        self.check_label_groups = (\n",
    "            check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS\n",
    "        )\n",
    "\n",
    "        supported_entities = supported_entities if supported_entities else self.ENTITIES\n",
    "\n",
    "        if model and model_path:\n",
    "            raise ValueError(\"Only one of model or model_path should be provided.\")\n",
    "        elif model and not model_path:\n",
    "            self.model = model\n",
    "        elif not model and model_path:\n",
    "            print(f\"Loading model from {model_path}\")\n",
    "            self.model = SequenceTagger.load(model_path)\n",
    "        else:\n",
    "            print(f\"Loading model for language {supported_language}\")\n",
    "            self.model = SequenceTagger.load(\n",
    "                self.MODEL_LANGUAGES.get(supported_language)\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            supported_entities=supported_entities,\n",
    "            supported_language=supported_language,\n",
    "            name=\"Flair Analytics\",\n",
    "        )\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_supported_entities(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return supported entities by this model.\n",
    "        :return: List of the supported entities.\n",
    "        \"\"\"\n",
    "        return self.supported_entities\n",
    "\n",
    "    # Class to use Flair with Presidio as an external recognizer.\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyze text using Text Analytics.\n",
    "        :param text: The text for analysis.\n",
    "        :param entities: Not working properly for this recognizer.\n",
    "        :param nlp_artifacts: Not used by this recognizer.\n",
    "        :param language: Text language. Supported languages in MODEL_LANGUAGES\n",
    "        :return: The list of Presidio RecognizerResult constructed from the recognized\n",
    "            Flair detections.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        sentences = Sentence(text)\n",
    "        self.model.predict(sentences)\n",
    "\n",
    "        # If there are no specific list of entities, we will look for all of it.\n",
    "        if not entities:\n",
    "            entities = self.supported_entities\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity not in self.supported_entities:\n",
    "                continue\n",
    "\n",
    "            for ent in sentences.get_spans(\"ner\"):\n",
    "                if not self.__check_label(\n",
    "                    entity, ent.labels[0].value, self.check_label_groups\n",
    "                ):\n",
    "                    continue\n",
    "                textual_explanation = self.DEFAULT_EXPLANATION.format(\n",
    "                    ent.labels[0].value\n",
    "                )\n",
    "                explanation = self.build_flair_explanation(\n",
    "                    round(ent.score, 2), textual_explanation\n",
    "                )\n",
    "                flair_result = self._convert_to_recognizer_result(ent, explanation)\n",
    "\n",
    "                results.append(flair_result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _convert_to_recognizer_result(self, entity, explanation) -> RecognizerResult:\n",
    "        entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)\n",
    "        flair_score = round(entity.score, 2)\n",
    "\n",
    "        flair_results = RecognizerResult(\n",
    "            entity_type=entity_type,\n",
    "            start=entity.start_position,\n",
    "            end=entity.end_position,\n",
    "            score=flair_score,\n",
    "            analysis_explanation=explanation,\n",
    "        )\n",
    "\n",
    "        return flair_results\n",
    "\n",
    "    def build_flair_explanation(\n",
    "        self, original_score: float, explanation: str\n",
    "    ) -> AnalysisExplanation:\n",
    "        \"\"\"\n",
    "        Create explanation for why this result was detected.\n",
    "        :param original_score: Score given by this recognizer\n",
    "        :param explanation: Explanation string\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        explanation = AnalysisExplanation(\n",
    "            recognizer=self.__class__.__name__,\n",
    "            original_score=original_score,\n",
    "            textual_explanation=explanation,\n",
    "        )\n",
    "        return explanation\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_label(\n",
    "        entity: str, label: str, check_label_groups: Tuple[Set, Set]\n",
    "    ) -> bool:\n",
    "        return any(\n",
    "            [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT92Kk44lgAV",
    "outputId": "0fc28bdc-4a3a-4e68-8617-27cdcedbc3ce"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from flair.data import Sentence\n",
    "    from flair.models import SequenceTagger\n",
    "\n",
    "    # load tagger\n",
    "    tagger = SequenceTagger.load(\"flair/ner-english-large\")\n",
    "\n",
    "    # make example sentence\n",
    "    sentence = Sentence(\"My name is Karishma Shirsath. I live in Toronto Canada.\")\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # print sentence\n",
    "    print(sentence)\n",
    "\n",
    "    # print predicted NER spans\n",
    "    print(\"The following NER tags are found:\")\n",
    "    # iterate over entities and print\n",
    "    for entity in sentence.get_spans(\"ner\"):\n",
    "        print(entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgYJJVilwbVF",
    "outputId": "20e52cfd-0e6e-4906-bcb0-3c403160293d"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from flair.data import Sentence\n",
    "    from flair.models import SequenceTagger\n",
    "\n",
    "    # load tagger\n",
    "    tagger = SequenceTagger.load(\"flair/ner-english-large\")\n",
    "\n",
    "    # make example sentence\n",
    "    sentence = Sentence(\"My name is Karishma Shirsath. I live in Toronto Canada.\")\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # print sentence\n",
    "    print(sentence)\n",
    "\n",
    "    # Anonymize identified named entities\n",
    "    anonymized_sentence = str(sentence)\n",
    "    for entity in sentence.get_spans(\"ner\"):\n",
    "        entity_text = entity.text\n",
    "        anonymized_text = \"*\" * len(entity_text)\n",
    "        anonymized_sentence = anonymized_sentence.replace(entity_text, anonymized_text)\n",
    "\n",
    "    # print anonymized sentence\n",
    "    print(\"Anonymized sentence:\")\n",
    "    print(anonymized_sentence)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
